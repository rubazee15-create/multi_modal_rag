{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7fafcba",
   "metadata": {},
   "source": [
    "Multi-Modal RAG with Images and Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65eb4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Dependencies: Poppler, Tesseract and libmagic installed globally (in bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948e12da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Ensuring the .venv has the necessary python bridges\n",
    "%pip install -Uq \"unstructured[all-docs]\"            # will install all the needed dependencies and handle any doc like pdf, csv, ppt etc.\n",
    "%pip install -Uq langchain langchain-community \n",
    "%pip install -Uq langchain-qdrant                    # The local vector store\n",
    "%pip install -Uq langchain-huggingface\n",
    "%pip install -Uq sentence-transformers \n",
    "%pip install -Uq qdrant-client\n",
    "%pip install -Uq langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ed34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-15 12:41:08.786683511 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Unstructured for document parsing\n",
    "from unstructured.partition.pdf import partition_pdf           # instead of pdf, we can type csv, ppt etc.\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# Langchain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b3a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning the PDF document: /home/ruba/Desktop/Multi-Modal RAG/docs/sensors-24-03064.pdf\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 678 atomic elements\n"
     ]
    }
   ],
   "source": [
    "# STEP 1. PARTITIONING THE PDF DOCUMENT INTO ATOMIC ELEMENTS - using the unstructured library\n",
    "def partition_document(file_path: str):\n",
    "    \"\"\"Extract atomic elements from the PDF using unstructured\"\"\"\n",
    "    print(f\"Partitioning the PDF document: {file_path}\")\n",
    "\n",
    "    elements = partition_pdf(\n",
    "        filename=file_path,                     # path to my PDF file\n",
    "        startegy=\"hi_res\",                      # the most accurate (but slower) processing method of extraction\n",
    "        infer_table_structure=True,             # keeps tables as structured HTML, not jumbled text\n",
    "        extract_image_block_types=[\"Image\"],    # grabs the images found in the PDF\n",
    "        extract_image_block_to_payload=True     # Stores/converts the images as 'image_base64' data that we can actually use - because that is how images are transferred on the internet\n",
    "    )\n",
    "\n",
    "    print(f\"Extracted {len(elements)} atomic elements\")\n",
    "    return elements\n",
    "\n",
    "# Testing with my PDF file\n",
    "file_path = \"/home/ruba/Desktop/Multi-Modal RAG/docs/sensors-24-03064.pdf\"   # copy path of where the PDF is stored \n",
    "elements = partition_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4626a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the quantity of atomic elements\n",
    "# len(elements)\n",
    "\n",
    "# Printing elements\n",
    "# elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b396a628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.FigureCaption'>\",\n",
       " \"<class 'unstructured.documents.elements.Formula'>\",\n",
       " \"<class 'unstructured.documents.elements.Header'>\",\n",
       " \"<class 'unstructured.documents.elements.Image'>\",\n",
       " \"<class 'unstructured.documents.elements.ListItem'>\",\n",
       " \"<class 'unstructured.documents.elements.NarrativeText'>\",\n",
       " \"<class 'unstructured.documents.elements.Table'>\",\n",
       " \"<class 'unstructured.documents.elements.Text'>\",\n",
       " \"<class 'unstructured.documents.elements.Title'>\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All types of different atomic elements we see from unstructured\n",
    "set([str(type(el)) for el in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b3a16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NarrativeText',\n",
       " 'element_id': '297786a844c4de05ec169354a953c893',\n",
       " 'text': 'Figure 7. Overview of the different reviewed UAV sensors.',\n",
       " 'metadata': {'detection_class_prob': 0.8115609884262085,\n",
       "  'is_extracted': 'true',\n",
       "  'coordinates': {'points': ((np.float64(460.86962890625),\n",
       "     np.float64(1475.6962844444445)),\n",
       "    (np.float64(460.86962890625), np.float64(1501.0014577777777)),\n",
       "    (np.float64(1112.1025390625), np.float64(1501.0014577777777)),\n",
       "    (np.float64(1112.1025390625), np.float64(1475.6962844444445))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1654,\n",
       "   'layout_height': 2339},\n",
       "  'last_modified': '2026-01-12T13:40:10',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 21,\n",
       "  'file_directory': '/home/ruba/Desktop/Multi-Modal RAG/docs',\n",
       "  'filename': 'sensors-24-03064.pdf'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting one random element out of the 678 elements\n",
    "elements[232].to_dict()         # .to_dict() prints all the info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38675389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'Image',\n",
       " 'element_id': '0fd0590c5ffaf709d366ddb00857ebf7',\n",
       " 'text': 'i | SENSOrs',\n",
       " 'metadata': {'detection_class_prob': 0.7279917597770691,\n",
       "  'coordinates': {'points': ((np.float64(89.6802749633789),\n",
       "     np.float64(134.87904357910156)),\n",
       "    (np.float64(89.6802749633789), np.float64(216.10633850097656)),\n",
       "    (np.float64(438.9463806152344), np.float64(216.10633850097656)),\n",
       "    (np.float64(438.9463806152344), np.float64(134.87904357910156))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1654,\n",
       "   'layout_height': 2339},\n",
       "  'last_modified': '2026-01-12T13:40:10',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 1,\n",
       "  'image_base64': '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',\n",
       "  'image_mime_type': 'image/jpeg',\n",
       "  'file_directory': '/home/ruba/Desktop/Multi-Modal RAG/docs',\n",
       "  'filename': 'sensors-24-03064.pdf'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the image atomic elements data (how it looks like)\n",
    "# Gathering all the images\n",
    "images = [element for element in elements if element.category =='Image']\n",
    "print(f\"Found {len(images)} images\")\n",
    "\n",
    "# Printing the first image\n",
    "images[0].to_dict()\n",
    "\n",
    "# 'image_base64' shown in the output below is the accurate raw image (use this), rather than 'text' in the ouutput (jumbled up, not reliable) - OCR jumbles up the text in the image\n",
    "# view the 'image_base64' (copy) on codebeautify.org (paste), and the actual image will appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142cc7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 tables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'Table',\n",
       " 'element_id': '0aafe84105e107fc419c3d93c0d05b7c',\n",
       " 'text': 'Advantages ✓ Informative scene data ✓ Anti-jamming ability ✓ Relatively high accuracy Vision-Based Navigation for UAVs Disadvantages Challenges Field of Application X Complex environment structures reflect complexities in the navigation algorithm Real-time processing requirements Agriculture X Performance is impacted by adverse weather conditions Integration with image-based sensing modalities Surveillance X Vulnerable to visual illusions Power consumption Environmental monitoring',\n",
       " 'metadata': {'detection_class_prob': 0.8665180802345276,\n",
       "  'is_extracted': 'true',\n",
       "  'coordinates': {'points': ((np.float64(114.76978302001953),\n",
       "     np.float64(1114.007568359375)),\n",
       "    (np.float64(114.76978302001953), np.float64(1390.7633056640625)),\n",
       "    (np.float64(1549.9464111328125), np.float64(1390.7633056640625)),\n",
       "    (np.float64(1549.9464111328125), np.float64(1114.007568359375))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1654,\n",
       "   'layout_height': 2339},\n",
       "  'last_modified': '2026-01-12T13:40:10',\n",
       "  'text_as_html': '<table><thead><tr><th>Advantages</th><th>Disadvantages</th><th>Challenges</th><th>Field of Application</th></tr></thead><tbody><tr><td>v Informative scene data</td><td>X Complex environment structures reflect complexities in ws : the navigation algorithm</td><td>Real-time processin eP 8 requirements</td><td>Agriculture</td></tr><tr><td>. . a: V Antijamming ability</td><td>X Perf is i ted b erformance is impacted by adverse weather conditions</td><td>Integrati ith i -based Integration with image-base: sensing modalities</td><td>: Surveillance</td></tr><tr><td>Vv Relatively high accuracy</td><td>X Vulnerable to visual illusions</td><td>Power consumption</td><td>Environmental monitoring</td></tr></tbody></table>',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 7,\n",
       "  'file_directory': '/home/ruba/Desktop/Multi-Modal RAG/docs',\n",
       "  'filename': 'sensors-24-03064.pdf',\n",
       "  'parent_id': '327f7f90102b09bc3861c19bf7aa1b97'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checing for table atomic elements \n",
    "# Gathering all tables\n",
    "tables = [element for element in elements if element.category == 'Table']\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "\n",
    "# Printing the first table\n",
    "tables[0].to_dict()\n",
    "\n",
    "# Use 'text_as_html' attribute (more reliable), rather than 'text' (jumbled up, not reliable) - OCR jumbles up the text in the table\n",
    "# Use jsfiddle.net to view text_as_html table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce665d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Chunks by Title...\n",
      "Created 75 chunks\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: CLUBBING ATOMIC ELEMENTS TOGETHER BY TITLE - Chunking by title\n",
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using the title-based startegy\"\"\"   # chunking by title prompt\n",
    "    print(\"Creating Chunks by Title...\")\n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,                        # the parsed (partitioned) PDF atomic elements from the previous step\n",
    "        max_characters=3000,             # Hard limit - never exceed 3000 characters per chunk!!!\n",
    "        new_after_n_chars=2400,          # Try to start a new chunk after 2400 characters\n",
    "        combine_text_under_n_chars=500   # Merge tiny chunks under 500 characters with neighbours\n",
    "    )\n",
    "\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Creating chunks\n",
    "chunks = create_chunks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93161a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the chunks created\n",
    "chunks\n",
    "\n",
    "# Checking the types of chunks that are created\n",
    "set([str(type(chunk)) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11cb8a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CompositeElement',\n",
       " 'element_id': '0c07bbfa-587f-4a59-839f-c030b9728311',\n",
       " 'text': '2 of 42\\n\\nSensors 2024, 24, 3064\\n\\nbetween a UAV and the payload (i.e., suspended loads), whereas in [47], the state-of-\\n\\nthe-art focused mainly on drone detection and classification techniques. From another perspective, the review in [48] identifies current gaps in the application of UAVs for the creation of 3D models in the contexts of urban planning and historic monuments preservation. The review in [49] took into consideration the diverse possible applications of drones in healthcare applications, whereas in [50], the main focus was on the study of marine mammals (i.e., individual estimation, body conditions and biometrics, behavioral patterns, etc.). On the other hand, the review in [51] examined the drone-integrated Geographic Information System (GIS) in different fields, differently from the work in [52], which focused on UAVs’ potential to advance climate change research and monitoring. Although the work in [53] mainly accounted for the UAV remote sensing of crop species, it included some of the multispectral sensors used in such applications, thus enriching the informative background about UAVs. In a more general approach, the review in [54] considered a structured presentation of the recent trends in the UAV field, classifying them according to their flight characteristics, showcasing the potential areas for further development, and addressing the hardware/software within a UAV. In [55], the research considered an overview of anti-collision technologies for UAVs with the associated types of sensors, whereas in [56], the focus was on UAV threat models, with security and privacy aspects. Divergently, in [57], the focus was on search and rescue operations driven by UAVs, whereas in [58], the review examined the deployment of UAVs for monitoring and inspection in the construction industry. Concerning the You Only Look Once (YOLO) algorithm used for real-time detection and classification of multiple targets, the work in [59] investigated the integration of YOLO with UAV technology and the corresponding practical applications (e.g., engineering, transportation, automation, etc.). Given that path planning holds significance in the context of drone autonomy, the authors of [60] reviewed the environmental representation as well as the path generation techniques for drones. Considering the potential of UAVs in warehouse management, a systematic literature review was conducted in [61], enriching the background of knowledge about the obstacles versus the adoption of UAVs in warehouse handling, unlike in [62], where the review primarily focused on the role of drones in flood management. Different UAV platforms for autonomous applications are reviewed in [63], which also presented the state-of-art and estimation techniques for UAVs, in addition to their correspondent flight phases.',\n",
       " 'metadata': {'file_directory': '/home/ruba/Desktop/Multi-Modal RAG/docs',\n",
       "  'filename': 'sensors-24-03064.pdf',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'last_modified': '2026-01-12T13:40:10',\n",
       "  'page_number': 2,\n",
       "  'orig_elements': 'eJzlWN+P2zYS/lcIPyWArdVPSspbrsHlCrSX4pIeUGyNBS1SFrEyqSMpO76g//vNULJXWi1at7i8tA+LXVHkcGa+b74Z7f2XlWjFQSj3IPnqDVnVUR0ltUhzTrNyl6URq8o0q+skT7OyLLPVmqwOwjHOHIP9X1aV1oZLxZyw/rllZ927h0bIfeNgJU6SEs6MyyfJXQOrEc1SWO20VA7P3d9HaUGDfE2iNAmS7Zo8LdByXMiyJMheWvBHYGVlz9aJA8bxg/ws2o8dq8TqF3jBhROVk1o9VC2z9qEzegfbwqCIs7SADbVsxQOXBnZpc0YLd40+iDvT79jdO2Efne7uvu9bJzffa85a8q+37++4ruxqPKzYQeAxK5TVxm7idBMmIU2DjteXPe7c+T2s61pZMfTnbnwt7YP47AyrnPA4ONOLlc+b2vds75N7vxJqv9r6VeseDprLWg7b4zCmmzDaRPGnKHmThm+iEE93cPJB9YedMAgFZsLBNf4E0TVJY9x1cesfgnHYCLues6KgUVaVBS9DkeV5JWLAhvKsoCKtw4JWX40VZRFQBDgMckR8fKQjRZKwCNIXnv32P8iHokzTvwgfkikfPg5+EjgJ8eMPensbPXZ5Tcsy4gndlZzFVcrSjLMw4WkaZ7s8/Wr0SGkcxOhtGcRIgMtzToNiVIgiiF5a8Cd+lSJ/Tga0Uj36k19W1jEDCCouPsMCzSdkSHPc25sWHyrpRPC3NN9cAylpTsN09ct2SSm/Yn6Hckw5uBPuJIQijPz49t+EKU5cI0jHzq1mnLySgQjWxPa2E+A0J7hqX6/JqRFGMEukAgagTuApiM6Jja43Uwr/qCDHYq+N/K/gn/DWF9hcsXRXV2XOwkKEVZanCbI7q8Ok5rSK6FdkcxSEwM0iCujAZv8cJVl+YS8tguSlleHMH5O8MqTRX4/vUVlOCV8sCV8sCY9kmZtJ6MxMuTRT3mImLaKJmSxcmMnCW8xQGk7NREsz0S1m8mJmJl6aiW8xU5SzoJKlmeQWM1EU06mddGknvclOkmdTO9nSTnaTnSyc+UOXduhtdmYUzJaam72guS/YoXkxtbOkcnYTlaMymtlZcjm7ictxlMcTO3RJZnoTmeNkRkO6ZDO9ic1xFk5xp0s605voHNNsijtd8pm+wOf/d4+E3rYBr0itq95CEzwwqdoz0Ypwo5UgV5n3/dNLPWjkILUEXjVK/qcXNiB/N/oAezQYNKQTBroqnDuKoX0acZTiNLTUYkskB3dRay2peoO+kz3rfMvF3RM5x88K6N4WHDT+XQW9+fIieUdAs0V7PVhphYFZfNmbHVOkA71XUu29+4200IxkBacgfXCrJZ0RVpijNxmQT89cLbfEaf0ID06jcQuOmzF22MohPmNhpNCQlV07c9z74HPovWsEa11TAU6zTfNpIwvHaQNRGCAhJ3g3Xmddz89o9sCMVLjrcGAQ/DjJALHkUfIeeqmwTh78DWuy03AIfOdycAsTsZPQih1kAhzYiYYdpTZwrGPOCYNeCVcFrwPyYbh4ALWBkws0s2hLxGd2AH+G8cqHvIGEiT1kChbfCw1/dQ2k/VsFKA5+kY9+sCCv3n/78TVagtZbC88EoEXLwYfrCvCxRnah+ZM2j8PFOKGfwGpz5S5YRar83MdhVAIoDkkGYQF2jB+ZqoAgLeYFfkMwewzECmbABCYFSCGRHmofkLcAlu73zbM7k+2lQFhV6V5hfBdi4oxpxAFuJVi1yDmAqjK6I1gLwHWAyIGdqu1x3LQAAe7weOMc5CsGYRiLnvig4F7bo4cz1rgGqCEUINjgRWhDXpJ7FGTHqse9AQc5YTuYEn1eAsg/zMIHDRzcCyXwKrBqNKuaJa7p9kp48IIB+UxfuR4ffM0ody3D4WiF2DnAi1/LEVPi0VxflOM8envwCcQxd4/wwIo0pG5xwkVs/NhmoFo9Q22jTxWzl0CfgGW+chCAujeeo1wcRas7rO21B5VxDt5ez4JpfoJjd1bXDv8gJ+kghcM3gs/QfZZtL+kY2TFNBEQMRe/zBKEzcGRT6baV9iqIutV7VDb0y0sXXjHomrW6kr4qcHL1EjGC/UwI6OjCTAMwna5B/Rtlbz2YtgJEVLqzD7gz8sgq+NvTCUB/hyK192W0HoznLxmfFALEXfXAzW7UOgtFDUYU2Z19QM98LbYz9szVQHStPiMcMyF/KjV/oVRDtwA3noR8INywxnt4OgfkGw1FbNQFzZ90DxIF1fgdivQHLPBXP3347sNrwlr8KnPNYSgivBMcbjcgi7/R01BesRo7kHPo13vh7PqZCkBXkOqIGrsfsPTVNwjeaAK9GMDxoF1ocb5+hQL5Ic0dyjIE0yHhwYN23kFeiWAPwg4fKZBQgekCVwyDbGnjRnlnvdMXqR9V+70HyzXAE1D05qkHNhqElVi5Vz5ezNe8cV57ljer9OE8xA5PDSoSvL6n4XaEegxdqKOEMwgyBGDETB6AJCfRtvh7+PgGfwbxeTZBeIyGfulx9gW3LPkLi8BvLF8QaYuNUMFINBY9GT5ZwX5FWpij4CZQrQs7ke3YDHv8LvRw0mi7fialE/mE+x6VPrWCQ7sYtNR3xB0Mc1ULbuME0A/BMa672cwycxLbZ+sh7FUrH8Vw+dDFoJqmJQQ1jC0e+95Ta/PvdSvmc0Xdas0nCcB6vzRSpB5g77AvDPkdUQV/5kRj1wRdkpJcuytMGPoi+SPi13+G4OSIjH6aN54jOugFCiwfBpAnuX8qAd/1B/HvGmaBAdP/svyTGePb2vAflu3/AJ1m7SQ='}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing at one particular chunk to check its properties\n",
    "chunks[5].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e0a544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x7164a02f12a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a02f1660>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a02f1840>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a02f1a20>,\n",
       " <unstructured.documents.elements.Image at 0x7164a02f1c00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a02f1de0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking all the atomic elements (Title, Narrative text, Text, Image, Table, Header etc.) in the chunk - using 'metadata' and 'orig_elements'\n",
    "# For better understanding selecting a chunk with image and table in the PDF\n",
    "\n",
    "# Checking atomic elements in a chunk with an Image\n",
    "chunks[1].metadata.orig_elements  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524381b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x7164a0148fa0>,\n",
       " <unstructured.documents.elements.Table at 0x7164a0149660>,\n",
       " <unstructured.documents.elements.ListItem at 0x7164a0149d50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a0149ff0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a014a1a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7164a014a380>,\n",
       " <unstructured.documents.elements.Header at 0x7164a0149300>,\n",
       " <unstructured.documents.elements.Header at 0x7164a0148d00>,\n",
       " <unstructured.documents.elements.ListItem at 0x7164a014a7a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x7164a014ac80>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking atomic elements in a chunk with a Table\n",
    "chunks[14].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b5be60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Table',\n",
       " 'element_id': '0aafe84105e107fc419c3d93c0d05b7c',\n",
       " 'text': 'Advantages ✓ Informative scene data ✓ Anti-jamming ability ✓ Relatively high accuracy Vision-Based Navigation for UAVs Disadvantages Challenges Field of Application X Complex environment structures reflect complexities in the navigation algorithm Real-time processing requirements Agriculture X Performance is impacted by adverse weather conditions Integration with image-based sensing modalities Surveillance X Vulnerable to visual illusions Power consumption Environmental monitoring',\n",
       " 'metadata': {'detection_class_prob': 0.8665180802345276,\n",
       "  'is_extracted': 'true',\n",
       "  'coordinates': {'points': ((np.float64(114.76978302001953),\n",
       "     np.float64(1114.007568359375)),\n",
       "    (np.float64(114.76978302001953), np.float64(1390.7633056640625)),\n",
       "    (np.float64(1549.9464111328125), np.float64(1390.7633056640625)),\n",
       "    (np.float64(1549.9464111328125), np.float64(1114.007568359375))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1654,\n",
       "   'layout_height': 2339},\n",
       "  'last_modified': '2026-01-12T13:40:10',\n",
       "  'text_as_html': '<table><thead><tr><th>Advantages</th><th>Disadvantages</th><th>Challenges</th><th>Field of Application</th></tr></thead><tbody><tr><td>v Informative scene data</td><td>X Complex environment structures reflect complexities in ws : the navigation algorithm</td><td>Real-time processin eP 8 requirements</td><td>Agriculture</td></tr><tr><td>. . a: V Antijamming ability</td><td>X Perf is i ted b erformance is impacted by adverse weather conditions</td><td>Integrati ith i -based Integration with image-base: sensing modalities</td><td>: Surveillance</td></tr><tr><td>Vv Relatively high accuracy</td><td>X Vulnerable to visual illusions</td><td>Power consumption</td><td>Environmental monitoring</td></tr></tbody></table>',\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 7,\n",
       "  'file_directory': '/home/ruba/Desktop/Multi-Modal RAG/docs',\n",
       "  'filename': 'sensors-24-03064.pdf',\n",
       "  'parent_id': '327f7f90102b09bc3861c19bf7aa1b97'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To further check the contents/properties of a specific atomic element within the chunk (eg. chunk 14) we use the following code:\n",
    "\n",
    "chunks[14].metadata.orig_elements[1].to_dict() # will print the info of the 2nd atomic element in chunk 14 i.e. Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15122ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunks with AI summaries...\n",
      " Processing Chunk 1/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 2\n",
      " Creating AI Summary for mixed content...\n",
      "AI summary failed: this model only supports one image while more than one image requested (status code: 500)\n",
      "  AI summary generated successfully\n",
      " AI summary failed: 'NoneType' object is not subscriptable\n",
      " Processing Chunk 2/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 1\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: The image displays a Creative Commons (CC) license icon, which is a symbol used to indicate that a work is licensed under a Creative Commons license. The icon is a white circle with a black border, co...\n",
      " Processing Chunk 3/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 4/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 5/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 6/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 7/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 8/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 9/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 10/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 2\n",
      " Creating AI Summary for mixed content...\n",
      "AI summary failed: this model only supports one image while more than one image requested (status code: 500)\n",
      "  AI summary generated successfully\n",
      " AI summary failed: 'NoneType' object is not subscriptable\n",
      " Processing Chunk 11/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 12/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 13/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 14/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 15/75\n",
      " Types found: ['table', 'text']\n",
      " Tables: 1, Images: 0\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: Here is a comprehensive, searchable description that covers the key facts, numbers, and data points from the text and tables, as well as the main topics and concepts discussed:\n",
      "\n",
      "**Vision-Based UAV Nav...\n",
      " Processing Chunk 16/75\n",
      " Types found: ['table', 'text']\n",
      " Tables: 1, Images: 0\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: Here is a comprehensive, searchable description that covers the key facts, numbers, and data points from the text and tables:\n",
      "\n",
      "**Nature-Inspired AI Algorithms for UAV Navigation**\n",
      "\n",
      "**Key Facts and Num...\n",
      " Processing Chunk 17/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 18/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 19/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 20/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 21/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 1\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: **Step 1: Identify the key components of the image.**\n",
      "\n",
      "The image shows a drone flying over a city, with a green box highlighting a specific building.\n",
      "\n",
      "**Step 2: Determine the purpose of the image.**\n",
      "\n",
      "...\n",
      " Processing Chunk 22/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 23/75\n",
      " Types found: ['table', 'image', 'text']\n",
      " Tables: 1, Images: 2\n",
      " Creating AI Summary for mixed content...\n",
      "AI summary failed: this model only supports one image while more than one image requested (status code: 500)\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: The consecutive YOLOv4 is mainly characterized by three grids as follows: backbone (utilizes the CSPDarnet53 classifier), neck (a parameter assembling approach within lessen the information trajectory...\n",
      " Processing Chunk 24/75\n",
      " Types found: ['table', 'text']\n",
      " Tables: 1, Images: 0\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: Here is a comprehensive, searchable description that covers the key facts, numbers, and data points from the text and tables, as well as the main topics and concepts discussed:\n",
      "\n",
      "**Key Facts, Numbers, ...\n",
      " Processing Chunk 25/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 26/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 1\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: The image presents a block diagram representation of a control system for a Unmanned Aerial Vehicle (UAV), which includes various components such as a flight controller, processing unit, sensors, and ...\n",
      " Processing Chunk 27/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 1\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: The text content is about the Odroid XU4 as an alternative to the Raspberry Pi as a UAV's SBC. The Odroid XU4 has a 2 GHz processor with four ARM Cortex-A15 cores and four ARM Cortex-A7 cores. It has ...\n",
      " Processing Chunk 28/75\n",
      " Types found: ['image', 'text']\n",
      " Tables: 0, Images: 1\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: The provided text appears to be a description of a system for object detection in drones, using a specific hardware platform called the “NVIDIA Jetson” (both “NVIDIA” and “JET” are trademarked by  [  ...\n",
      " Processing Chunk 29/75\n",
      " Types found: ['table', 'text']\n",
      " Tables: 1, Images: 0\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: Here is a comprehensive, searchable description that covers the key facts, numbers, and data points from the text and tables, as well as the main topics and concepts discussed:\n",
      "\n",
      "**System-on-Module (So...\n",
      " Processing Chunk 30/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 31/75\n",
      " Types found: ['table', 'text']\n",
      " Tables: 1, Images: 0\n",
      " Creating AI Summary for mixed content...\n",
      "  AI summary generated successfully\n",
      " Enhanced content preview: Here is a comprehensive, searchable description that covers the key facts, numbers, and data points from the text and tables, as well as the main topics and concepts discussed:\n",
      "\n",
      "**System-on-Chip (SoC)...\n",
      " Processing Chunk 32/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 33/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 34/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 35/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 36/75\n",
      " Types found: ['text']\n",
      " Tables: 0, Images: 0\n",
      " using raw text(no tables/images\n",
      " Processing Chunk 37/75\n",
      " Types found: ['table', 'image', 'text']\n",
      " Tables: 1, Images: 1\n",
      " Creating AI Summary for mixed content...\n"
     ]
    }
   ],
   "source": [
    "# STEP 3. SUMMARIZING THE CHUNKS AND CONVERTING IT INTO LANGCHAIN DOCUMENTS\n",
    "# Substep 1: Separate content types - for every single chunk, we will separate all the tables, images and text \n",
    "def separate_content_types(chunk):\n",
    "    \"\"\"Analyze what types of content are in a chunk\"\"\"\n",
    "    content_data= {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "    \n",
    "    # Checking for tables ang images in original elements\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "\n",
    "            # Handling tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html= getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "\n",
    "            # Handling images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "\n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n",
    "\n",
    "# As there are tables and images involved - we will have to summarize the text\n",
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content using Llama 3.2 Vision\"\"\"\n",
    "\n",
    "    try: \n",
    "        # 1. Initializing LLM\n",
    "        llm = ChatOllama(model=\"llama3.2-vision\", temperature=0)     # llama 3.2-version model also converts the images into text (11B)\n",
    "\n",
    "        # 2. Creating the base prompt\n",
    "        prompt = f\"\"\"You are creating a searchable description for document content retrieval. \n",
    "\n",
    "        Content to analyze:\n",
    "        Text content:\n",
    "        {text}\n",
    "        \\n\"\"\"\n",
    "\n",
    "        # 3. Adding tables if present\n",
    "        if tables: \n",
    "            prompt += \"TABLES:\\n\"  \n",
    "            for i, table in enumerate(tables):\n",
    "                prompt += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "\n",
    "                # 4. Adding the Task Instructions to the prompt\n",
    "                prompt += \"\"\" \n",
    "                Your TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "                1. Key facts, numbers, and data points from text and tables.\n",
    "                2. Main topics and concepts discussed.\n",
    "                3. Questions this content could answer.\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images).\n",
    "                5. Alternative search terms users might use.\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # 5. Building message content starting with the text/table prompt\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "\n",
    "        # 6. Adding images to the message - appending the images that are in image_base64\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\", \n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}  # message_content has 2 items in the List i.e. type and image_url that will be provided in the HumanMessage\n",
    "            })\n",
    "\n",
    "        # 7. Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])               # The LLM will look at all the text that includes the base 64 images and tables, then it will give us a summary\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e: \n",
    "        print(f\"AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables: \n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "            if images:\n",
    "                summary += f\" [Contains {len(images)} images(s)]\"\n",
    "                return summary\n",
    "            \n",
    "def summarize_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI summaries\"\"\"\n",
    "    print(\"Processing chunks with AI summaries...\")\n",
    "\n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\" Processing Chunk {current_chunk}/{total_chunks}\")\n",
    "\n",
    "\n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "\n",
    "        # Debugging prints\n",
    "        print(f\" Types found: {content_data['types']}\")\n",
    "        print(f\" Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "\n",
    "        # Creating AI-enhanced summary if the chunk has tables or images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\" Creating AI Summary for mixed content...\")\n",
    "            try: \n",
    "                enhanced_content = create_ai_enhanced_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'],\n",
    "                    content_data['images'],\n",
    "                )\n",
    "\n",
    "                print(f\"  AI summary generated successfully\")\n",
    "                print(f\" Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\" AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\" using raw text(no tables/images\")\n",
    "            enhanced_content = content_data['text']\n",
    "\n",
    "    # Creating LangChain Document with rich metadata -LangChain Document contains page_content and metadata\n",
    "    doc = Document(\n",
    "        page_content=enhanced_content,\n",
    "        metadata={\n",
    "            \"original_content\": json.dumps({           # converting the entire content into json\n",
    "                \"raw_text\": content_data['text'],           # raw text\n",
    "                \"tables_html\": content_data['tables'],      # raw tables\n",
    "                \"images_base64\": content_data['images']     # raw images\n",
    "            })\n",
    "        }\n",
    "    )\n",
    "\n",
    "    langchain_documents.append(doc)      # appending all the documents \n",
    "\n",
    "    print(f\" Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents           # returning a list of 75 LangChain documents (as we had 75 chunks)\n",
    "\n",
    "# Processing chunks with AI\n",
    "processed_chunks = summarize_chunks(chunks)                                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a08cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ed9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f19a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
